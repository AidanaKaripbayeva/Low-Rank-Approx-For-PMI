{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "import gensim\n",
    "import scipy.sparse\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy import sparse\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.sparse.linalg import svds, eigs\n",
    "from numpy.linalg import solve, norm\n",
    "from numpy.random import rand\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "\n",
    "rng = np.random\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "config.gpu_options.allow_growth = True #allocate dynamically \n",
    "sess = tf.Session(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "vocab_size = 180000\n",
    "window_size = 3\n",
    "k = 5 #size of the negative sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_download(filename, url, path):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    local_filename = os.path.join(path, filename)\n",
    "    if not os.path.exists(local_filename):      \n",
    "        local_filename, _ = urllib.request.urlretrieve(url + filename,\n",
    "                                                   local_filename)\n",
    "    statinfo = os.stat(local_filename)\n",
    "    return local_filename\n",
    "\n",
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0]))\n",
    "    return data\n",
    "\n",
    "def clean_data():\n",
    "    filename = maybe_download('text8.zip', 'http://mattmahoney.net/dc/', '/home/alena/')\n",
    "    vocabulary = read_data(filename)\n",
    "    words = vocabulary.split()\n",
    "\n",
    "    # convert to lower case\n",
    "    words = [word.lower() for word in words]\n",
    "\n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in words]\n",
    "\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    print(len(words))\n",
    "    \n",
    "    return words\n",
    "\n",
    "def build_dataset(words, vocab_sizze):  \n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocab_sizze-1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005207\n"
     ]
    }
   ],
   "source": [
    "data, count, dictionary, reversed_dictionary = build_dataset(clean_data(), vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_bigrams(data, window_sizze):\n",
    "    for idx in range(len(data)):\n",
    "        \n",
    "        if idx==1:   \n",
    "            window = data[idx-1:idx+window_sizze]\n",
    "            w = window[1]\n",
    "            for i in range(len(window)):\n",
    "                if i == 1:\n",
    "                    continue\n",
    "                yield(w, window[i])\n",
    "           \n",
    "                \n",
    "        elif idx == 0:\n",
    "            window = data[idx: idx + window_sizze]\n",
    "            w = window[0]\n",
    "            for i in range(len(window)):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                yield(w, window[i])\n",
    "         \n",
    "                \n",
    "        else:\n",
    "            window = data[idx-2: idx+window_sizze]\n",
    "            w = window[2]\n",
    "            for i in range(len(window)):\n",
    "                if i == 2:\n",
    "                    continue\n",
    "                yield(w, window[i])\n",
    "           \n",
    "        \n",
    "\n",
    "def construct_vocab(data):\n",
    "    vocab = Counter()\n",
    "    \n",
    "    for (w1, w2) in gen_bigrams(data, window_size): # count 1gram & 2gram\n",
    "        vocab.update([w1, w2, (w1, w2)])\n",
    "    return vocab\n",
    "        \n",
    "\n",
    "def calc_pmi(vocab, det):\n",
    "    \n",
    "    for (w1, w2) in filter(lambda el: isinstance(el, tuple), vocab):    \n",
    "        p_a, p_b = float(vocab[w1]), float(vocab[w2])\n",
    "        p_ab = float(vocab[(w1, w2)])\n",
    "        pmi = log((det * p_ab) / (p_a * p_b), 2)\n",
    "        sppmi = max(pmi - log(k,10), 0)\n",
    "        sppmi=sppmi/2\n",
    "        yield (w1, w2, sppmi)\n",
    "\n",
    "vocab = construct_vocab(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_det(vocabb):   \n",
    "    det = 0.0\n",
    "    for (w1,w2) in  filter(lambda el: isinstance(el, tuple), vocabb):\n",
    "        det = det + float(vocabb[(w1,w2)])\n",
    "    return det\n",
    "\n",
    "def constr_sparse_pmi():\n",
    "    \n",
    "    row=[]\n",
    "    column=[]\n",
    "    data = []\n",
    "\n",
    "    for (w1,w2, sppmi) in calc_pmi(vocab, calc_det(vocab)):\n",
    "        row.append(w1)\n",
    "        column.append(w2)\n",
    "        data.append(sppmi)\n",
    "        row.append(w2)\n",
    "        column.append(w1)\n",
    "        data.append(sppmi)\n",
    "    sparse_pmi = sparse.csr_matrix((data, (row, column)), shape=(len(dictionary), len(dictionary)))\n",
    "    \n",
    "    return sparse_pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparse_pmim = constr_sparse_pmi()\n",
    "sparse.save_npz(\"pmi_k\"+str(k)+\"_\"+str(vocab_size)+\".npz\", sparse_pmim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def col():\n",
    "    column = []\n",
    "    for i in reversed_dictionary:\n",
    "        column.append(reversed_dictionary[i])\n",
    "    my_col = pd.DataFrame(column)\n",
    "    return my_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_data = col()\n",
    "my_data.to_csv(\"c_k\"+str(k)+\"_\"+str(vocab_size), sep=\" \", index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
